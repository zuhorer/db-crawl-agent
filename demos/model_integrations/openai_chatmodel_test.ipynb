{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab2b197",
   "metadata": {},
   "source": [
    "\n",
    "# OpenAI ChatModel Integration — Test Notebook\n",
    "\n",
    "This notebook verifies your `ChatModel` abstraction using the **OpenAI** implementation (`OpenAIChat`).  \n",
    "It includes smoke tests for: basic chat, streaming, JSON mode, and tool-calls.  \n",
    "> **Prereqs**: You have your package installed (e.g., `pip install -e .`) and an OpenAI API key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8196038",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Environment & Package Setup\n",
    "\n",
    "- Ensure you are at your project root (where `pyproject.toml` lives).\n",
    "- (Once) install your package locally:  \n",
    "  ```bash\n",
    "  pip install -e .\n",
    "  ```\n",
    "- Make sure you have the OpenAI SDK installed (your `pyproject.toml` should include `openai>=1.45.0`).  \n",
    "  Otherwise:\n",
    "  ```bash\n",
    "  pip install openai>=1.45.0\n",
    "  ```\n",
    "- Provide your API Key (either export the env var before launching Jupyter, or set it in the cell below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b635fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY set: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Option A: already set in your shell\n",
    "# os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Option B: set here for this notebook session\n",
    "os.environ['OPENAI_API_KEY'] = \"put your open ai api key here\"\n",
    "\n",
    "print(\"OPENAI_API_KEY set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc3b6d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c141f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.db_crawl_agents.llms.openai_integration.chat_openai import OpenAIChat\n",
    "from src.db_crawl_agents.utils.types import ChatMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b3096",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Basic Chat (non-streaming)\n",
    "\n",
    "This will call your `OpenAIChat.chat(...)` method with a tiny prompt and print the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6c28ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL  : gpt-4o-2024-08-06\n",
      "FINISH : stop\n",
      "USAGE  : {'completion_tokens': 68, 'prompt_tokens': 25, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n",
      "\n",
      "ASSISTANT:\n",
      " 1. Bengaluru, often referred to as the \"Silicon Valley of India,\" is a major hub for information technology and innovation, hosting numerous tech companies and startups.\n",
      "\n",
      "2. The city is known for its pleasant climate, with moderate temperatures throughout the year, earning it the nickname \"Garden City\" due to its many parks and green spaces.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chat = OpenAIChat(model=\"gpt-4o\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a concise assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Give me two short facts about Bengaluru.\")\n",
    "]\n",
    "\n",
    "resp = chat.chat(messages, temperature=0.2, max_tokens=150)\n",
    "print(\"MODEL  :\", resp.model)\n",
    "print(\"FINISH :\", resp.finish_reason)\n",
    "print(\"USAGE  :\", resp.usage)\n",
    "print(\"\\nASSISTANT:\\n\", resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b872df3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Streaming Chat\n",
    "\n",
    "This yields token deltas in real time and then prints the final aggregated text once the stream ends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f916a",
   "metadata": {},
   "source": [
    "raise issue: streaming is not functional right now I will need to debug this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae198a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAM (tokens): "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Responses' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTREAM (tokens):\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m final_text = []\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Intermediate deltas have small 'content' chunks; final one has the full aggregate\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/agent-database-crawler/src/db_crawl_agents/llms/openai_integration/chat_openai.py:204\u001b[39m, in \u001b[36mOpenAIChat.stream\u001b[39m\u001b[34m(self, messages, temperature, max_tokens, top_p, stop, tools, tool_choice, response_format, metadata)\u001b[39m\n\u001b[32m    201\u001b[39m model_name = \u001b[38;5;28mself\u001b[39m._model\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_convert_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX-Client-Meta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdb_crawl_agents/openai_chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m    218\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    219\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m event.type == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: 'Responses' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "chat = OpenAIChat(model=\"gpt-4o\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a streaming assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Stream one short paragraph about Hampi hospitality and architecture.\")\n",
    "]\n",
    "\n",
    "print(\"STREAM (tokens):\", end=\" \")\n",
    "final_text = []\n",
    "for delta in chat.stream(messages, temperature=0.2, max_tokens=120):\n",
    "    # Intermediate deltas have small 'content' chunks; final one has the full aggregate\n",
    "    print(delta.content, end=\"\", flush=True)\n",
    "    final_text.append(delta.content)\n",
    "print(\"\\n\\nFINAL:\")\n",
    "print(\"\".join(final_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb0578",
   "metadata": {},
   "source": [
    "\n",
    "## 5) JSON Mode (structured output)\n",
    "\n",
    "Request a strict JSON object using `response_format={\"type\": \"json_object\"}`.  \n",
    "The model should return a valid JSON string in `resp.content`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc17947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW: {\n",
      "  \"city\": \"Mysuru\",\n",
      "  \"highlights\": [\n",
      "    \"Mysore Palace\",\n",
      "    \"Brindavan Gardens\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Parsed JSON: {'city': 'Mysuru', 'highlights': ['Mysore Palace', 'Brindavan Gardens']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "chat = OpenAIChat(model=\"gpt-4o\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a JSON-only assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Return a JSON with keys: city, highlights (list of 2 strings) for 'Mysuru'.\")\n",
    "]\n",
    "\n",
    "resp = chat.chat(\n",
    "    messages,\n",
    "    temperature=0.0,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(\"RAW:\", resp.content)\n",
    "try:\n",
    "    obj = json.loads(resp.content)\n",
    "    print(\"\\nParsed JSON:\", obj)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"❌ JSON parse error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7e98e",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Tool Calling (function-calling)\n",
    "\n",
    "We define a simple tool schema and ask the model to call it.  \n",
    "> **Note**: This only checks that the model **returns** a tool call; it does **not** execute the tool.  \n",
    "You can wire an execution loop in your app to run the tool and then provide a `tool` message back to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed0231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: \n",
      "TOOL CALLS: [ToolCall(id='call_BILCjxsxnzJXgWU1YnBqtPTp', type='function', function_name='get_weather', arguments_json='{\"city\":\"Bengaluru\"}')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a simple tool/function for the model to call\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a city (mock).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "chat = OpenAIChat(model=\"gpt-4o\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant. Use tools if needed.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What's the weather in Bengaluru right now?\")\n",
    "]\n",
    "\n",
    "resp = chat.chat(\n",
    "    messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"ASSISTANT:\", resp.content)\n",
    "print(\"TOOL CALLS:\", resp.tool_calls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c1002",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Round-trip: Provide a tool result back to the model\n",
    "\n",
    "If the previous cell returned a function call, you would:\n",
    "1. Parse `resp.tool_calls[0].arguments_json`  \n",
    "2. Execute your function (server-side)  \n",
    "3. Add a **tool** message with the result and call the model again\n",
    "\n",
    "Below is a mock example to illustrate that pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbda0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Bengaluru is 29°C and partly cloudy.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from typing import Optional\n",
    "\n",
    "def execute_get_weather(city: str) -> str:\n",
    "    # Mock result (replace with real integration)\n",
    "    return f\"It is 29°C and partly cloudy in {city}.\"\n",
    "\n",
    "def to_openai_tool_calls(tool_calls):\n",
    "    return [{\n",
    "        \"id\": tc.id,\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\"name\": tc.function_name, \"arguments\": tc.arguments_json or \"{}\"},\n",
    "    } for tc in (tool_calls or [])]\n",
    "\n",
    "\n",
    "assistant_with_calls = ChatMessage(\n",
    "    role=\"assistant\",\n",
    "    content=resp.content or \"\",\n",
    "    tool_calls=to_openai_tool_calls(resp.tool_calls)  # ✅ put calls here\n",
    ")\n",
    "followup_messages = messages + [assistant_with_calls]\n",
    "for tc in resp.tool_calls or []:\n",
    "    args = json.loads(tc.arguments_json or \"{}\")\n",
    "    result = execute_get_weather(args.get(\"city\", \"unknown\")) if tc.function_name == \"get_weather\" else \"Unknown tool\"\n",
    "    followup_messages.append(\n",
    "        ChatMessage(role=\"tool\", content=result, name=tc.function_name, tool_call_id=tc.id)\n",
    "    )\n",
    "\n",
    "final_resp = chat.chat(followup_messages)\n",
    "print(final_resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02aae72",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ✅ Done\n",
    "\n",
    "You just validated:\n",
    "- Basic chat (`chat`)\n",
    "- Streaming (`stream`)\n",
    "- JSON mode (`response_format={\"type\":\"json_object\"}`)\n",
    "- Tool calling round-trip pattern\n",
    "\n",
    "If you want, duplicate this notebook and swap in other providers (Anthropic, Groq, Bedrock) using the same `ChatModel` interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f9884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.107.2\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(openai.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
