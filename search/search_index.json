{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<ul> <li>llm integration </li> <li> <p>llm provider api, </p> </li> <li> <p>runnable llm adapter</p> </li> <li>runnable chat model</li> <li>chat_openai</li> <li></li> </ul> <p>right now my tool call integration is with openai llms which wraps </p> <p>testing</p>"},{"location":"guides/open_ai_chat_model_integration/","title":"OpenAI Chat Model Integration","text":"<p>This module provides an OpenAI-backed implementation of the <code>ChatModel</code> interface used by db-crawl agents.</p> <p>It enables conversational LLM interactions with support for tool calling, structured outputs, and normalized error handling, while keeping the rest of the system independent of OpenAI-specific APIs.</p>"},{"location":"guides/open_ai_chat_model_integration/#module-overview","title":"Module Overview","text":"<p>Module path: <code>db_crawl_agents.llm.openai_chat</code> Primary class: <code>OpenAIChat</code></p> <p>This integration wraps the OpenAI Python SDK and adapts it to db-crawl\u2019s internal message and response abstractions.</p>"},{"location":"guides/open_ai_chat_model_integration/#design-goals","title":"Design Goals","text":"<ul> <li>Isolate OpenAI-specific SDK usage</li> <li>Preserve a stable internal interface (<code>ChatModel</code>)</li> <li>Support agentic workflows (tool calling, retries, multi-step execution)</li> <li>Normalize errors across providers</li> <li>Allow easy substitution with other LLM providers (e.g. Gemini)</li> </ul>"},{"location":"guides/open_ai_chat_model_integration/#public-api","title":"Public API","text":""},{"location":"guides/open_ai_chat_model_integration/#openaichat","title":"<code>OpenAIChat</code>","text":"<p>```python class OpenAIChat(ChatModel):</p>"}]}